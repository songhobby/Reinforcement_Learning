{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "k7qS2DywTrj4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class MDP:\n",
    "    '''A simple MDP class.  It includes the following members'''\n",
    "\n",
    "    def __init__(self,T,R,discount):\n",
    "        '''Constructor for the MDP class\n",
    "\n",
    "        Inputs:\n",
    "        T -- Transition function: |A| x |S| x |S'| array\n",
    "        R -- Reward function: |A| x |S| array\n",
    "        discount -- discount factor: scalar in [0,1)\n",
    "\n",
    "        The constructor verifies that the inputs are valid and sets\n",
    "        corresponding variables in a MDP object'''\n",
    "\n",
    "        assert T.ndim == 3, \"Invalid transition function: it should have 3 dimensions\"\n",
    "        self.nActions = T.shape[0]\n",
    "        self.nStates = T.shape[1]\n",
    "        assert T.shape == (self.nActions,self.nStates,self.nStates), \"Invalid transition function: it has dimensionality \" + repr(T.shape) + \", but it should be (nActions,nStates,nStates)\"\n",
    "        assert (abs(T.sum(2)-1) < 1e-5).all(), \"Invalid transition function: some transition probability does not equal 1\"\n",
    "        self.T = T\n",
    "        assert R.ndim == 2, \"Invalid reward function: it should have 2 dimensions\" \n",
    "        assert R.shape == (self.nActions,self.nStates), \"Invalid reward function: it has dimensionality \" + repr(R.shape) + \", but it should be (nActions,nStates)\"\n",
    "        self.R = R\n",
    "        assert 0 <= discount < 1, \"Invalid discount factor: it should be in [0,1)\"\n",
    "        self.discount = discount\n",
    "        \n",
    "    def valueIteration(self,initialV,nIterations=np.inf,tolerance=0.01):\n",
    "        '''Value iteration procedure\n",
    "        V <-- max_a R^a + gamma T^a V\n",
    "\n",
    "        Inputs:\n",
    "        initialV -- Initial value function: array of |S| entries\n",
    "        nIterations -- limit on the # of iterations: scalar (default: infinity)\n",
    "        tolerance -- threshold on ||V^n-V^n+1||_inf: scalar (default: 0.01)\n",
    "\n",
    "        Outputs: \n",
    "        V -- Value function: array of |S| entries\n",
    "        iterId -- # of iterations performed: scalar\n",
    "        epsilon -- ||V^n-V^n+1||_inf: scalar'''\n",
    "        \n",
    "        # temporary values to ensure that the code compiles until this\n",
    "        # function is coded\n",
    "        V = initialV\n",
    "        iterId = 0\n",
    "        epsilon = np.inf\n",
    "        \n",
    "        while (iterId < nIterations and epsilon > tolerance):\n",
    "          iterId += 1\n",
    "          V_matrix = []\n",
    "          for i in range(self.nActions):\n",
    "            V_matrix.append(self.R[i]+self.discount*self.T[i] @ V)\n",
    "          V_next = np.amax(np.array(V_matrix), 0)\n",
    "          epsilon = np.max(np.abs(V_next - V))\n",
    "          V = V_next\n",
    "          \n",
    "        return [V,iterId,epsilon]\n",
    "          \n",
    "\n",
    "    def extractPolicy(self,V):\n",
    "        '''Procedure to extract a policy from a value function\n",
    "        pi <-- argmax_a R^a + gamma T^a V\n",
    "\n",
    "        Inputs:\n",
    "        V -- Value function: array of |S| entries\n",
    "\n",
    "        Output:\n",
    "        policy -- Policy: array of |S| entries'''\n",
    "\n",
    "        # temporary values to ensure that the code compiles until this\n",
    "        # function is coded\n",
    "        policy = np.zeros(self.nStates)\n",
    "        \n",
    "        V_matrix = []\n",
    "        for i in range(self.nActions):\n",
    "          V_matrix.append(self.R[i]+self.discount*self.T[i] @ V)\n",
    "        policy = np.array(V_matrix).argmax(0)\n",
    "\n",
    "        return policy \n",
    "\n",
    "    def evaluatePolicy(self,policy):\n",
    "        '''Evaluate a policy by solving a system of linear equations\n",
    "        V^pi = R^pi + gamma T^pi V^pi\n",
    "\n",
    "        Input:\n",
    "        policy -- Policy: array of |S| entries\n",
    "\n",
    "        Ouput:\n",
    "        V -- Value function: array of |S| entries'''\n",
    "\n",
    "        # temporary values to ensure that the code compiles until this\n",
    "        # function is coded\n",
    "        V = np.zeros(self.nStates)\n",
    "        \n",
    "        R_pi = np.zeros(self.nStates)\n",
    "        for i in range(self.nStates):\n",
    "          R_pi[i] = self.R[policy[i]][i]\n",
    "        \n",
    "        T_pi = np.zeros([self.nStates,self.nStates])\n",
    "        for i in range(self.nStates):\n",
    "          T_pi[i] = self.T[policy[i]][i]\n",
    "        \n",
    "        V = np.linalg.pinv(np.identity(self.nStates) - self.discount*T_pi) @ R_pi\n",
    "\n",
    "        return V\n",
    "        \n",
    "    def policyIteration(self,initialPolicy,nIterations=np.inf):\n",
    "        '''Policy iteration procedure: alternate between policy\n",
    "        evaluation (solve V^pi = R^pi + gamma T^pi V^pi) and policy\n",
    "        improvement (pi <-- argmax_a R^a + gamma T^a V^pi).\n",
    "\n",
    "        Inputs:\n",
    "        initialPolicy -- Initial policy: array of |S| entries\n",
    "        nIterations -- limit on # of iterations: scalar (default: inf)\n",
    "\n",
    "        Outputs: \n",
    "        policy -- Policy: array of |S| entries\n",
    "        V -- Value function: array of |S| entries\n",
    "        iterId -- # of iterations peformed by modified policy iteration: scalar\n",
    "        epsilon -- ||V^n-V^n+1||_inf: scalar'''\n",
    "\n",
    "        # temporary values to ensure that the code compiles until this\n",
    "        # function is coded\n",
    "        policy = initialPolicy\n",
    "        V = np.zeros(self.nStates)\n",
    "        iterId = 0\n",
    "        \n",
    "        while(iterId < nIterations):\n",
    "          iterId += 1\n",
    "          V = self.evaluatePolicy(policy)\n",
    "          policy_next = self.extractPolicy(V)\n",
    "          if np.array_equal(policy, policy_next):\n",
    "            return [policy,V,iterId]\n",
    "          policy = policy_next\n",
    "          \n",
    "        return [policy, V, iterId]\n",
    "          \n",
    "    def evaluatePolicyPartially(self,policy,initialV,nIterations=np.inf,tolerance=0.01):\n",
    "        '''Partial policy evaluation:\n",
    "        Repeat V^pi <-- R^pi + gamma T^pi V^pi\n",
    "\n",
    "        Inputs:\n",
    "        policy -- Policy: array of |S| entries\n",
    "        initialV -- Initial value function: array of |S| entries\n",
    "        nIterations -- limit on the # of iterations: scalar (default: infinity)\n",
    "        tolerance -- threshold on ||V^n-V^n+1||_inf: scalar (default: 0.01)\n",
    "\n",
    "        Outputs: \n",
    "        V -- Value function: array of |S| entries\n",
    "        iterId -- # of iterations performed: scalar\n",
    "        epsilon -- ||V^n-V^n+1||_inf: scalar'''\n",
    "\n",
    "        # temporary values to ensure that the code compiles until this\n",
    "        # function is coded\n",
    "        V = initialV\n",
    "        iterId = 0\n",
    "        epsilon = np.inf\n",
    "        \n",
    "        R_pi = np.zeros(self.nStates)\n",
    "        for i in range(self.nStates):\n",
    "          R_pi[i] = self.R[policy[i]][i]\n",
    "        \n",
    "        T_pi = np.zeros([self.nStates,self.nStates])\n",
    "        for i in range(self.nStates):\n",
    "          T_pi[i] = self.T[policy[i]][i]\n",
    "          \n",
    "        while(iterId < nIterations and epsilon > tolerance):\n",
    "          iterId += 1\n",
    "          V_next = R_pi + self.discount*T_pi @ V\n",
    "          epsilon = np.max(np.abs(V_next - V))\n",
    "          V = V_next\n",
    "\n",
    "        return [V,iterId,epsilon]\n",
    "\n",
    "    def modifiedPolicyIteration(self,initialPolicy,initialV,nEvalIterations=5,nIterations=np.inf,tolerance=0.01):\n",
    "        '''Modified policy iteration procedure: alternate between\n",
    "        partial policy evaluation (repeat a few times V^pi <-- R^pi + gamma T^pi V^pi)\n",
    "        and policy improvement (pi <-- argmax_a R^a + gamma T^a V^pi)\n",
    "\n",
    "        Inputs:\n",
    "        initialPolicy -- Initial policy: array of |S| entries\n",
    "        initialV -- Initial value function: array of |S| entries\n",
    "        nEvalIterations -- limit on # of iterations to be performed in each partial policy evaluation: scalar (default: 5)\n",
    "        nIterations -- limit on # of iterations to be performed in modified policy iteration: scalar (default: inf)\n",
    "        tolerance -- threshold on ||V^n-V^n+1||_inf: scalar (default: 0.01)\n",
    "\n",
    "        Outputs: \n",
    "        policy -- Policy: array of |S| entries\n",
    "        V -- Value function: array of |S| entries\n",
    "        iterId -- # of iterations peformed by modified policy iteration: scalar\n",
    "        epsilon -- ||V^n-V^n+1||_inf: scalar'''\n",
    "\n",
    "        # temporary values to ensure that the code compiles until this\n",
    "        # function is coded\n",
    "        policy = initialPolicy\n",
    "        V = initialV\n",
    "        iterId = 0\n",
    "        epsilon = np.inf\n",
    "        \n",
    "        while(iterId < nIterations and epsilon > tolerance):\n",
    "          iterId += 1\n",
    "          [V, _, _] = self.evaluatePolicyPartially(policy,V,nEvalIterations,tolerance=0.01)\n",
    "          policy = self.extractPolicy(V)\n",
    "          [V_next, _, _] = self.valueIteration(V,1)\n",
    "          epsilon = np.max(np.abs(V_next - V))\n",
    "          \n",
    "\n",
    "        return [policy,V,iterId,epsilon]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 333,
     "status": "ok",
     "timestamp": 1527922244811,
     "user": {
      "displayName": "Aaron Song",
      "photoUrl": "//lh3.googleusercontent.com/-RV7e7XLMjyc/AAAAAAAAAAI/AAAAAAAAABM/Tbm4mobW24o/s50-c-k-no/photo.jpg",
      "userId": "101268156466578523343"
     },
     "user_tz": 240
    },
    "id": "fr-7LEWkVxVH",
    "outputId": "589a265b-2c2d-4924-e5ae-48a8b9692efb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration:\n",
      "Value function [31.49636306 38.51527513 43.935435   54.1128575 ]\n",
      "Number of Iteration 58\n",
      "Policy [0 1 1 1]\n",
      "\n",
      "Policy Iteration:\n",
      "Value function [31.58510431 38.60401638 44.02417625 54.20159875]\n",
      "Number of Iteration 2\n",
      "Policy [0 1 1 1]\n",
      "\n",
      "Modified Policy Iteration:\n",
      "Value function [31.49636306 38.51527513 43.935435   54.1128575 ]\n",
      "Number of Iteration 13\n",
      "Policy [0 1 1 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' Construct simple MDP as described in Lecture 2a Slides 13-14'''\n",
    "# Transition function: |A| x |S| x |S'| array\n",
    "T = np.array([[[0.5,0.5,0,0],[0,1,0,0],[0.5,0.5,0,0],[0,1,0,0]],[[1,0,0,0],[0.5,0,0,0.5],[0.5,0,0.5,0],[0,0,0.5,0.5]]])\n",
    "# Reward function: |A| x |S| array\n",
    "R = np.array([[0,0,10,10],[0,0,10,10]])\n",
    "# Discount factor: scalar in [0,1)\n",
    "discount = 0.9        \n",
    "# MDP object\n",
    "mdp = MDP(T,R,discount)\n",
    "\n",
    "'''Test each procedure'''\n",
    "[V,nIterations,epsilon] = mdp.valueIteration(initialV=np.zeros(mdp.nStates))\n",
    "policy = mdp.extractPolicy(V)\n",
    "print(\"Value Iteration:\\nValue function {}\\nNumber of Iteration {}\\nPolicy {}\\n\".format(V,nIterations,policy))\n",
    "#V = mdp.evaluatePolicy(np.array([1,0,1,0]))\n",
    "[policy,V,iterId] = mdp.policyIteration(np.array([0,0,0,0]))\n",
    "print(\"Policy Iteration:\\nValue function {}\\nNumber of Iteration {}\\nPolicy {}\\n\".format(V,iterId,policy))\n",
    "#[V,iterId,epsilon] = mdp.evaluatePolicyPartially(np.array([1,0,1,0]),np.array([0,10,0,13]))\n",
    "\n",
    "[policy,V,iterId,tolerance] = mdp.modifiedPolicyIteration(np.array([0,0,0,0]),np.array([0,0,0,0]))\n",
    "print(\"Modified Policy Iteration:\\nValue function {}\\nNumber of Iteration {}\\nPolicy {}\\n\".format(V,iterId,policy))\n",
    "\n",
    "iterId_lst=[]\n",
    "for i in range(10):\n",
    "  [_,_,iterId,_] = mdp.modifiedPolicyIteration(np.zeros(mdp.nStates,dtype=np.int),np.zeros(mdp.nStates),nEvalIterations=i+1)\n",
    "  iterId_lst.append(iterId)\n",
    "  \n",
    "plt.title(\"number of iterations required when k increases\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"number of iterations\")\n",
    "_=plt.plot(np.arange(10)+1, iterId_lst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WnqGiIngPFJQ"
   },
   "source": [
    "##Analysis for the impact of the number of iterations in partial policy evaluation on the results\n",
    "As the number of iterations in partial policy evaluation increases, the modified policy iteration converges faster but the rate slows down and eventaully reaches a plain. when k is small the modified policy iteration approaches value iteration and when k is large, the modified policy iteration approaches policy iteration as the partial policy evaluation approaches true policy evaluation of policy iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zL50Xjb-cfHp"
   },
   "outputs": [],
   "source": [
    "class RL:\n",
    "    def __init__(self,mdp,sampleReward):\n",
    "        '''Constructor for the RL class\n",
    "\n",
    "        Inputs:\n",
    "        mdp -- Markov decision process (T, R, discount)\n",
    "        sampleReward -- Function to sample rewards (e.g., bernoulli, Gaussian).\n",
    "        This function takes one argument: the mean of the distributon and \n",
    "        returns a sample from the distribution.\n",
    "        '''\n",
    "\n",
    "        self.mdp = mdp\n",
    "        self.sampleReward = sampleReward\n",
    "\n",
    "    def sampleRewardAndNextState(self,state,action):\n",
    "        '''Procedure to sample a reward and the next state\n",
    "        reward ~ Pr(r)\n",
    "        nextState ~ Pr(s'|s,a)\n",
    "\n",
    "        Inputs:\n",
    "        state -- current state\n",
    "        action -- action to be executed\n",
    "\n",
    "        Outputs: \n",
    "        reward -- sampled reward\n",
    "        nextState -- sampled next state\n",
    "        '''\n",
    "\n",
    "        reward = self.sampleReward(self.mdp.R[action,state])\n",
    "        cumProb = np.cumsum(self.mdp.T[action,state,:])\n",
    "        nextState = np.where(cumProb >= np.random.rand(1))[0][0]\n",
    "        return [reward,nextState]\n",
    "\n",
    "    def qLearning(self,s0,initialQ,nEpisodes,nSteps,epsilon=0,temperature=1):\n",
    "        '''qLearning algorithm.  Epsilon exploration and Boltzmann exploration\n",
    "        are combined in one procedure by sampling a random action with \n",
    "        probabilty epsilon and performing Boltzmann exploration otherwise.  \n",
    "        When epsilon and temperature are set to 0, there is no exploration.\n",
    "\n",
    "        Inputs:\n",
    "        s0 -- initial state\n",
    "        initialQ -- initial Q function (|A|x|S| array)\n",
    "        nEpisodes -- # of episodes (one episode consists of a trajectory of nSteps that starts in s0\n",
    "        nSteps -- # of steps per episode\n",
    "        epsilon -- probability with which an action is chosen at random\n",
    "        temperature -- parameter that regulates Boltzmann exploration\n",
    "\n",
    "        Outputs: \n",
    "        Q -- final Q function (|A|x|S| array)\n",
    "        policy -- final policy\n",
    "        '''\n",
    "\n",
    "        # temporary values to ensure that the code compiles until this\n",
    "        # function is coded\n",
    "        Q = initialQ\n",
    "        n_table = np.zeros(Q.shape,dtype=int)\n",
    "        learning_rate = 0\n",
    "        episodeId = 0\n",
    "        reward_episodes = []\n",
    "        while (episodeId < nEpisodes):\n",
    "          episodeId += 1\n",
    "          s=s0\n",
    "          reward_cum=0\n",
    "          stepId = 0\n",
    "          while (stepId < nSteps):\n",
    "            stepId += 1\n",
    "            action = 0\n",
    "            if (np.random.rand(1) < epsilon):\n",
    "              action = np.random.randint(self.mdp.nActions)\n",
    "            else:\n",
    "              boltz_state = np.exp(Q[:,s].flatten() / temperature)\n",
    "              boltz_state = boltz_state / boltz_state.sum()\n",
    "              boltz_state = np.cumsum(boltz_state)\n",
    "              action = np.where(boltz_state >= np.random.rand(1))[0][0]\n",
    "            [reward, s_next] = self.sampleRewardAndNextState(s,action)\n",
    "            n_table[action,s] += 1\n",
    "            learning_rate = 1 / n_table[action,s]\n",
    "            Q[action,s] = Q[action,s] + learning_rate*(reward + self.mdp.discount*np.max(Q[:,s_next].flatten())-Q[action,s])\n",
    "            s = s_next\n",
    "            reward_cum += reward\n",
    "          reward_episodes.append(reward_cum)\n",
    "        \n",
    "        policy = Q.argmax(0).flatten()\n",
    "\n",
    "        return [Q,policy, reward_episodes]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "3RB30toU7kU6",
    "outputId": "701d9abc-2f7e-4863-fca7-c300df188e07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------[3 3 3 1 3 3 3 1 2 1 0 1 3 3 3 0 2]\n",
      "-------------------------------------------------------"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "''' Construct a simple maze MDP\n",
    "\n",
    "  Grid world layout:\n",
    "\n",
    "  ---------------------\n",
    "  |  0 |  1 |  2 |  3 |\n",
    "  ---------------------\n",
    "  |  4 |  5 |  6 |  7 |\n",
    "  ---------------------\n",
    "  |  8 |  9 | 10 | 11 |\n",
    "  ---------------------\n",
    "  | 12 | 13 | 14 | 15 |\n",
    "  ---------------------\n",
    "\n",
    "  Goal state: 15 \n",
    "  Bad state: 9\n",
    "  End state: 16\n",
    "\n",
    "  The end state is an absorbing state that the agent transitions \n",
    "  to after visiting the goal state.\n",
    "\n",
    "  There are 17 states in total (including the end state) \n",
    "  and 4 actions (up, down, left, right).'''\n",
    "\n",
    "# Transition function: |A| x |S| x |S'| array\n",
    "T = np.zeros([4,17,17])\n",
    "a = 0.8;  # intended move\n",
    "b = 0.1;  # lateral move\n",
    "\n",
    "# up (a = 0)\n",
    "\n",
    "T[0,0,0] = a+b;\n",
    "T[0,0,1] = b;\n",
    "\n",
    "T[0,1,0] = b;\n",
    "T[0,1,1] = a;\n",
    "T[0,1,2] = b;\n",
    "\n",
    "T[0,2,1] = b;\n",
    "T[0,2,2] = a;\n",
    "T[0,2,3] = b;\n",
    "\n",
    "T[0,3,2] = b;\n",
    "T[0,3,3] = a+b;\n",
    "\n",
    "T[0,4,4] = b;\n",
    "T[0,4,0] = a;\n",
    "T[0,4,5] = b;\n",
    "\n",
    "T[0,5,4] = b;\n",
    "T[0,5,1] = a;\n",
    "T[0,5,6] = b;\n",
    "\n",
    "T[0,6,5] = b;\n",
    "T[0,6,2] = a;\n",
    "T[0,6,7] = b;\n",
    "\n",
    "T[0,7,6] = b;\n",
    "T[0,7,3] = a;\n",
    "T[0,7,7] = b;\n",
    "\n",
    "T[0,8,8] = b;\n",
    "T[0,8,4] = a;\n",
    "T[0,8,9] = b;\n",
    "\n",
    "T[0,9,8] = b;\n",
    "T[0,9,5] = a;\n",
    "T[0,9,10] = b;\n",
    "\n",
    "T[0,10,9] = b;\n",
    "T[0,10,6] = a;\n",
    "T[0,10,11] = b;\n",
    "\n",
    "T[0,11,10] = b;\n",
    "T[0,11,7] = a;\n",
    "T[0,11,11] = b;\n",
    "\n",
    "T[0,12,12] = b;\n",
    "T[0,12,8] = a;\n",
    "T[0,12,13] = b;\n",
    "\n",
    "T[0,13,12] = b;\n",
    "T[0,13,9] = a;\n",
    "T[0,13,14] = b;\n",
    "\n",
    "T[0,14,13] = b;\n",
    "T[0,14,10] = a;\n",
    "T[0,14,15] = b;\n",
    "\n",
    "T[0,15,16] = 1;\n",
    "T[0,16,16] = 1;\n",
    "\n",
    "# down (a = 1)\n",
    "\n",
    "T[1,0,0] = b;\n",
    "T[1,0,4] = a;\n",
    "T[1,0,1] = b;\n",
    "\n",
    "T[1,1,0] = b;\n",
    "T[1,1,5] = a;\n",
    "T[1,1,2] = b;\n",
    "\n",
    "T[1,2,1] = b;\n",
    "T[1,2,6] = a;\n",
    "T[1,2,3] = b;\n",
    "\n",
    "T[1,3,2] = b;\n",
    "T[1,3,7] = a;\n",
    "T[1,3,3] = b;\n",
    "\n",
    "T[1,4,4] = b;\n",
    "T[1,4,8] = a;\n",
    "T[1,4,5] = b;\n",
    "\n",
    "T[1,5,4] = b;\n",
    "T[1,5,9] = a;\n",
    "T[1,5,6] = b;\n",
    "\n",
    "T[1,6,5] = b;\n",
    "T[1,6,10] = a;\n",
    "T[1,6,7] = b;\n",
    "\n",
    "T[1,7,6] = b;\n",
    "T[1,7,11] = a;\n",
    "T[1,7,7] = b;\n",
    "\n",
    "T[1,8,8] = b;\n",
    "T[1,8,12] = a;\n",
    "T[1,8,9] = b;\n",
    "\n",
    "T[1,9,8] = b;\n",
    "T[1,9,13] = a;\n",
    "T[1,9,10] = b;\n",
    "\n",
    "T[1,10,9] = b;\n",
    "T[1,10,14] = a;\n",
    "T[1,10,11] = b;\n",
    "\n",
    "T[1,11,10] = b;\n",
    "T[1,11,15] = a;\n",
    "T[1,11,11] = b;\n",
    "\n",
    "T[1,12,12] = a+b;\n",
    "T[1,12,13] = b;\n",
    "\n",
    "T[1,13,12] = b;\n",
    "T[1,13,13] = a;\n",
    "T[1,13,14] = b;\n",
    "\n",
    "T[1,14,13] = b;\n",
    "T[1,14,14] = a;\n",
    "T[1,14,15] = b;\n",
    "\n",
    "T[1,15,16] = 1;\n",
    "T[1,16,16] = 1;\n",
    "\n",
    "# left (a = 2)\n",
    "\n",
    "T[2,0,0] = a+b;\n",
    "T[2,0,4] = b;\n",
    "\n",
    "T[2,1,1] = b;\n",
    "T[2,1,0] = a;\n",
    "T[2,1,5] = b;\n",
    "\n",
    "T[2,2,2] = b;\n",
    "T[2,2,1] = a;\n",
    "T[2,2,6] = b;\n",
    "\n",
    "T[2,3,3] = b;\n",
    "T[2,3,2] = a;\n",
    "T[2,3,7] = b;\n",
    "\n",
    "T[2,4,0] = b;\n",
    "T[2,4,4] = a;\n",
    "T[2,4,8] = b;\n",
    "\n",
    "T[2,5,1] = b;\n",
    "T[2,5,4] = a;\n",
    "T[2,5,9] = b;\n",
    "\n",
    "T[2,6,2] = b;\n",
    "T[2,6,5] = a;\n",
    "T[2,6,10] = b;\n",
    "\n",
    "T[2,7,3] = b;\n",
    "T[2,7,6] = a;\n",
    "T[2,7,11] = b;\n",
    "\n",
    "T[2,8,4] = b;\n",
    "T[2,8,8] = a;\n",
    "T[2,8,12] = b;\n",
    "\n",
    "T[2,9,5] = b;\n",
    "T[2,9,8] = a;\n",
    "T[2,9,13] = b;\n",
    "\n",
    "T[2,10,6] = b;\n",
    "T[2,10,9] = a;\n",
    "T[2,10,14] = b;\n",
    "\n",
    "T[2,11,7] = b;\n",
    "T[2,11,10] = a;\n",
    "T[2,11,15] = b;\n",
    "\n",
    "T[2,12,8] = b;\n",
    "T[2,12,12] = a+b;\n",
    "\n",
    "T[2,13,9] = b;\n",
    "T[2,13,12] = a;\n",
    "T[2,13,13] = b;\n",
    "\n",
    "T[2,14,10] = b;\n",
    "T[2,14,13] = a;\n",
    "T[2,14,14] = b;\n",
    "\n",
    "T[2,15,16] = 1;\n",
    "T[2,16,16] = 1;\n",
    "\n",
    "# right (a = 3)\n",
    "\n",
    "T[3,0,0] = b;\n",
    "T[3,0,1] = a;\n",
    "T[3,0,4] = b;\n",
    "\n",
    "T[3,1,1] = b;\n",
    "T[3,1,2] = a;\n",
    "T[3,1,5] = b;\n",
    "\n",
    "T[3,2,2] = b;\n",
    "T[3,2,3] = a;\n",
    "T[3,2,6] = b;\n",
    "\n",
    "T[3,3,3] = a+b;\n",
    "T[3,3,7] = b;\n",
    "\n",
    "T[3,4,0] = b;\n",
    "T[3,4,5] = a;\n",
    "T[3,4,8] = b;\n",
    "\n",
    "T[3,5,1] = b;\n",
    "T[3,5,6] = a;\n",
    "T[3,5,9] = b;\n",
    "\n",
    "T[3,6,2] = b;\n",
    "T[3,6,7] = a;\n",
    "T[3,6,10] = b;\n",
    "\n",
    "T[3,7,3] = b;\n",
    "T[3,7,7] = a;\n",
    "T[3,7,11] = b;\n",
    "\n",
    "T[3,8,4] = b;\n",
    "T[3,8,9] = a;\n",
    "T[3,8,12] = b;\n",
    "\n",
    "T[3,9,5] = b;\n",
    "T[3,9,10] = a;\n",
    "T[3,9,13] = b;\n",
    "\n",
    "T[3,10,6] = b;\n",
    "T[3,10,11] = a;\n",
    "T[3,10,14] = b;\n",
    "\n",
    "T[3,11,7] = b;\n",
    "T[3,11,11] = a;\n",
    "T[3,11,15] = b;\n",
    "\n",
    "T[3,12,8] = b;\n",
    "T[3,12,13] = a;\n",
    "T[3,12,12] = b;\n",
    "\n",
    "T[3,13,9] = b;\n",
    "T[3,13,14] = a;\n",
    "T[3,13,13] = b;\n",
    "\n",
    "T[3,14,10] = b;\n",
    "T[3,14,15] = a;\n",
    "T[3,14,14] = b;\n",
    "\n",
    "T[3,15,16] = 1;\n",
    "T[3,16,16] = 1;\n",
    "\n",
    "# Reward function: |A| x |S| array\n",
    "R = -1 * np.ones([4,17]);\n",
    "\n",
    "# set rewards\n",
    "R[:,15] = 100;  # goal state\n",
    "R[:,9] = -70;   # bad state\n",
    "R[:,16] = 0;    # end state\n",
    "\n",
    "# Discount factor: scalar in [0,1)\n",
    "discount = 0.95\n",
    "        \n",
    "# MDP object\n",
    "mdp = MDP(T,R,discount)\n",
    "\n",
    "# RL problem\n",
    "rlProblem = RL(mdp,np.random.normal)\n",
    "\n",
    "# Test Q-learning\n",
    "plt.figure(figsize=(20,15))\n",
    "history_lst=[]\n",
    "policy = []\n",
    "for i in range(100):\n",
    "  print('-',end='')\n",
    "  [Q,policy,history] = rlProblem.qLearning(s0=0,initialQ=np.zeros([mdp.nActions,mdp.nStates]),nEpisodes=200,nSteps=100,epsilon=0.05)\n",
    "  history_lst.append(history)\n",
    "_=plt.plot(range(200),np.average(history_lst,axis=0),'-b', label='epsilon=0.05')\n",
    "print(policy)\n",
    "\n",
    "history_lst=[]\n",
    "for i in range(100):\n",
    "  print('-',end='')\n",
    "  [Q,policy,history] = rlProblem.qLearning(s0=0,initialQ=np.zeros([mdp.nActions,mdp.nStates]),nEpisodes=200,nSteps=100,epsilon=0.1)\n",
    "  history_lst.append(history)\n",
    "_=plt.plot(range(200),np.average(history_lst,axis=0),'-r', label='epsilon=0.1')\n",
    "print(policy)\n",
    "\n",
    "history_lst=[]\n",
    "for i in range(100):\n",
    "  print('-',end='')\n",
    "  [Q,policy,history] = rlProblem.qLearning(s0=0,initialQ=np.zeros([mdp.nActions,mdp.nStates]),nEpisodes=200,nSteps=100,epsilon=0.3)\n",
    "  history_lst.append(history)\n",
    "_=plt.plot(range(200),np.average(history_lst,axis=0),'-c', label='epsilon=0.3')\n",
    "print(policy)\n",
    "\n",
    "history_lst=[]\n",
    "for i in range(100):\n",
    "  print('-',end='')\n",
    "  [Q,policy,history] = rlProblem.qLearning(s0=0,initialQ=np.zeros([mdp.nActions,mdp.nStates]),nEpisodes=200,nSteps=100,epsilon=0.5)\n",
    "  history_lst.append(history)\n",
    "_=plt.plot(range(200),np.average(history_lst,axis=0),'-y', label='epsilon=0.5')\n",
    "print(policy)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"episodes\")\n",
    "plt.ylabel(\"cumulative rewards\")\n",
    "plt.title(\"RL learning as epsilon varies\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "AYsZSr3TKQay"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Apcdnab6ZPII"
   },
   "source": [
    "##The impact of the exploration probability epsilon on the cumulative rewards per episode earned during training\n",
    "As shown in the graph above, at the beginning of the training before the cumulative rewards converge to the optimal value, larger epsilon or larger exploration rate speed up the convergence to optimal cumulative rewards while smaller exploration rate might lead to sub-optimal solutions. Therefore, smaller epsilon leads to less cumulative rewards and the variance of the cumulative rewards tend to be larger. After learning from enough episodes, models with different epsilon (as long as epsilon is not zero) will all converge to the optimal solution, but the model with larger epsilon keep exploring more after convergence resulting in the larger variance in the cumulative rewards after convergence. Eventually all the models with epsilon greater than zero and less than 1 will converge to the optimal solution. The cumulative reward is also a good indicator of the Q values during learning as the convergence of Q values results in the convergence of policy and thus the rewards obtained in a stochastic manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4800,
     "status": "ok",
     "timestamp": 1527893578919,
     "user": {
      "displayName": "Aaron Song",
      "photoUrl": "//lh3.googleusercontent.com/-RV7e7XLMjyc/AAAAAAAAAAI/AAAAAAAAABM/Tbm4mobW24o/s50-c-k-no/photo.jpg",
      "userId": "101268156466578523343"
     },
     "user_tz": 240
    },
    "id": "0LcicXDZjRQu",
    "outputId": "9ae435f7-d828-49cd-843b-a809469ac40e"
   },
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rT-Ye0wtoDG5"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    plt.imshow(env.render('mode=rgb_array'))\n",
    "    env.step(env.action_space.sample()) # take a random action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1355,
     "status": "ok",
     "timestamp": 1527816269520,
     "user": {
      "displayName": "Aaron Song",
      "photoUrl": "//lh3.googleusercontent.com/-RV7e7XLMjyc/AAAAAAAAAAI/AAAAAAAAABM/Tbm4mobW24o/s50-c-k-no/photo.jpg",
      "userId": "101268156466578523343"
     },
     "user_tz": 240
    },
    "id": "zdXK3aM7n55y",
    "outputId": "fc6cddc6-0dca-4d20-a53b-d409d5c9db5e"
   },
   "outputs": [],
   "source": [
    "cat /etc/issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-eM-x6CG8ikc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "RLa1.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
